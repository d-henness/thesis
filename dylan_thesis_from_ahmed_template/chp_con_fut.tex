\chapter{Conclusions and Future Work}

In this thesis I have illustrated how even small problems in computational quantum chemistry can benefit from parallel computation. I have done this both in the internal sense, where I rewrote the DFRATOM program to calculate the electronic structure of atoms on GPUs, and in the external sense, where I designed an algorithm that very effectively optimizes basis sets. This chapter will be broken up into two sections: the first which covers the rewriting of DFRATOM into cudaDFRATOM, and the second which summarizes the results of basis set optimization using cudaDFRATOM and rwtbs.

\section{cudaDFRATOM}
By harnessing the power of GPUs, I was able get the cudaDFRATOM program to accelerate the biggest bottleneck of these calculations, the evaluation of the two-electron integrals, by almost a factor of 20. This changes the bottleneck to be the matrix operations required in SCF. As a result, the total speedup is only about 2-4 times faster than the original DFRATOM program. While this is not as much of an increase as we originally hoped, it is still a very big improvement on the original program.

It is also worth pointing out that as the problems get larger, the speedup gets larger as well. This indicates that the algorithms written for this program could potentially work even better on even larger problems such as molecules. Modifications required to make the algorithms work on molecules are as follows. 

One could implement direct SCF. As the amount of two-electron integrals grows, it becomes impractical to try and store them all in memory. Instead, in the direct SCF algorithm, the integrals are calculated and consumed as needed. The current program has the following flow: the threads are mapped to all the integrals, all the integrals are calculated and stored in memory, and then all the integrals are used to form the \textbf{P} and \textbf{Q} matrices. If we wish to reuse as much of the code I've written as possible, the direct SCF implementation would like like this. First we would map the threads to a chunk of the integrals, then we could calculate only that chunk, and then we would build up only the elements of the \textbf{P} and \textbf{Q} that use those integrals. Now, we no longer need these integrals, so we can throw them away and start the calculation of a new chunk. We would then loop over these steps until the the \textbf{P} and \textbf{Q} matrices have been completely calculated. The mapping takes very little time to calculate, so the fact that we are calling it so many times would have relatively little impact on performance. 

One could also add some prescreening of the integrals to determine a rough estimate of their order of magnitude. If a particular integral is very close to zero, the effect it has on the calculation is relatively negligible. Therefore we could skip the calculation of this integral, and there would be almost no impact on the accuracy of the calculation. Due to how GPUs perform the calculations in warps, we would need to presort the integrals so that the ones that are likely to be close to zero are grouped together in order to maximize the returns of this procedure.

One would also need to alter the mapping algorithm so that it can handle calculations of different point groups. Atoms have spherical symmetry, meaning we can eliminate the need to calculate a lot of the integrals because many of them will be symmetrically the same. But this would not be the case in systems that have no symmetry, so we would need to change the mapping accordingly. This would actually be very simple to change, we would just need to write nearly identical code for each symmetry would would want to support.

One final thing to comment on in regards to cudaDFRATOM is the algorithm to form the \textbf{P} and \textbf{Q} matrices. I am still not completely satisfied with the current state of this part of the code. It is quite complicated and inelegant. It would seem to me that there should be a better way of performing this calculation, but so far I have been unable to improve upon the version that appears in this thesis. One possible thing would be to break up the calculation of the matrices into two different subroutines: one that calculates \textbf{P} and one that calculates \textbf{Q}. The calculation of \textbf{P} can be reformed into a simple matrix multiplication problem, but \textbf{Q} requires many conditional statements as a result of having to deal with the math of open shells. I did not try this as I believed the main bottleneck was reading the two-electron integrals. Therefore, I only wanted to have to read them once to reduce the amount of global memory reads as possible. But it might be the case that specialized code for each matrix could outperform generalized code for both.

\section{Basis Set Optimization}
This thesis also covered the optimization of new basis sets. I used a novel algorithm that almost entirely eliminates the need for human involvement in the optimization process. It produces useful accurate basis sets, that are practically guarantied to be as small as possible. I have optimized non-relativistic basis sets for the elements helium through radon. I have also optimized relativistic basis sets for the elements caesium through radon. Future work in the area would be the optimization of relativistic basis sets for the rest of the periodic table. Care must be taken when optimizing sets for the seventh row elements, because the assumption made for kinetic balancing begins to break down there. As a result, early attempts to optimize basis sets for these elements have proven to be prone to prolapsing. As a result, the method of automating the optimization process should be used with caution in this range, as the results it produces may erroneously appear to successful. 

Further work that could be done includes modifications to the cudaDFRATOM program to support basis sets other than WTBS, and also to include other optimization algorithms than \notetodylan{special font: newuoa}, which might be less prone to generating prolapsing basis sets. cudaDFRATOM also includes the option to optimize a different set of $\zeta$s for different groups of spinor symmetries. Therefore, it might be best to try and optimize basis sets that have grouped the s and p type spinors into one group and the d and f type spinors in another. The logic of this is that relativity has different effects on these two groups, so it might be best to try and separate the two so that each gets its own set of specialized basis functions.